# Real-Time Data Pipeline Platform

An interactive web platform to monitor and analyze real-time data from the Kafka → Spark Streaming → Delta Lake pipeline.

## 🚀 Key Features

### Real-Time Updates
- **Live Log Stream**: Real-time view of logs generated by the pipeline
- **Dynamic Metrics**: Automatic refresh of metrics based on real data
- **Interactive Charts**: Charts update on new incoming data
- **SSE Connection**: Server-Sent Events for instant updates

### Real Pipeline Data
- **Kafka Integration**: Direct connection to Kafka topics
- **Spark SQL Queries**: Execute queries against Delta Lake data
- **Delta Lake Analytics**: Query Delta tables directly
- **No Mock Data**: All data comes from the real pipeline

### Interactive Interface
- **Connection Status**: Visual indicators of pipeline health
- **Query Builder**: UI to run custom Spark SQL queries
- **Advanced Filters**: Filter logs by level, source, endpoint
- **Anomaly Detection**: Automatic anomaly detection on data

## 🏗️ Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Web Logs      │───▶│     Kafka       │───▶│  Spark Streaming│
│   (Sources)     │    │   (Broker)      │    │   (Processing)  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                                       │
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Web Platform  │◀───│   API Server    │◀───│   Delta Lake    │
│   (React App)   │    │   (Express)     │    │   (Storage)     │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

## 🛠️ Technologies

### Frontend
- **React 18** with TypeScript
- **Vite** for build and development
- **shadcn/ui** for UI components
- **Tailwind CSS** for styling
- **Recharts** for interactive charts
- **React Query** for state management

### Backend
- **Node.js** with Express
- **KafkaJS** for Kafka connectivity
- **Server-Sent Events** for real-time streaming
- **REST API** for queries and metrics

### Data Pipeline
- **Apache Kafka** for message streaming
- **Apache Spark** for processing
- **Delta Lake** for storage
- **Spark SQL** for querying

## 📦 Installation & Setup

### Prerequisites
- Node.js 18+ and npm
- Apache Kafka (running on localhost:9092)
- Apache Spark (running on localhost:7077)
- Delta Lake tables configured

### 1. Clone the Repository
```bash
git clone <YOUR_REPO_URL>
cd data-pipeline-platform
```

### 2. Install Dependencies
```bash
npm install
```

### 3. Configure Environment Variables
```bash
cp .env.example .env
```

Modifica il file `.env` con le tue configurazioni:
```env
# API Configuration
VITE_API_URL=http://localhost:4000

# Kafka Configuration
KAFKA_BROKERS=localhost:9092
KAFKA_TOPIC=web-logs
KAFKA_GROUP_ID=ui-bridge-group

# Spark Configuration
SPARK_MASTER=spark://localhost:7077
DELTA_LAKE_PATH=/tmp/delta-lake

# Server Configuration
PORT=4000
```

### 4. Start the Backend Server
```bash
npm run server
```

### 5. Start the Frontend App
```bash
npm run dev
```

## 🔧 Pipeline Configuration

### 1. Setup Kafka
Ensure Kafka is running and the `web-logs` topic exists:
```bash
# Crea il topic se non esiste
kafka-topics.sh --create --topic web-logs --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
```

### 2. Setup Spark
Start Spark with Delta Lake support:
```bash
spark-submit --packages io.delta:delta-core_2.12:2.4.0 \
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  your-spark-job.py
```

### 3. Test Log Generation
To test the platform, you can generate test logs to be processed by the pipeline:

```python
# Esempio di script Python per generare log
import json
import time
import random
from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda x: json.dumps(x).encode('utf-8')
)

endpoints = ['/api/users', '/api/orders', '/api/products', '/health', '/metrics']
methods = ['GET', 'POST', 'PUT', 'DELETE']

while True:
    log_entry = {
        'timestamp': time.time() * 1000,
        'method': random.choice(methods),
        'endpoint': random.choice(endpoints),
        'status_code': random.choice([200, 200, 200, 404, 500]),
        'response_time': random.randint(50, 2000),
        'ip': f'192.168.{random.randint(1,10)}.{random.randint(1,255)}',
        'user_id': f'user_{random.randint(1,1000)}',
        'session_id': f'sess_{random.randint(1000,9999)}'
    }
    
    producer.send('web-logs', log_entry)
    time.sleep(random.uniform(0.1, 2.0))
```

## 🎯 Platform Usage

### 1. Main Dashboard
- **Metrics Grid**: Real-time metrics (events/sec, error rate, response time, etc.)
- **Analytics Charts**: Interactive charts for traffic and performance
- **Live Log Stream**: Real-time log stream from the pipeline

### 2. Query Interface
- **Spark SQL**: Run custom queries on Delta Lake data
- **Sample Queries**: Predefined queries for common analyses
- **Real-time Results**: Live-updating results

### 3. Monitoring
- **Connection Status**: Visual indicators of pipeline status
- **Anomaly Detection**: Automatic error and anomaly detection
- **Performance Metrics**: Real-time performance monitoring

## 🔍 Sample Queries

### Analisi Errori
```sql
SELECT endpoint, count(*) as error_count,
       avg(response_time) as avg_response_time
FROM delta_lake.logs 
WHERE status >= 400 AND timestamp >= current_timestamp() - interval 1 hour
GROUP BY endpoint
ORDER BY error_count DESC
LIMIT 5
```

### Analisi Sessioni Utente
```sql
SELECT 
  user_id,
  count(distinct session_id) as sessions,
  count(*) as page_views,
  sum(response_time) / count(*) as avg_session_time
FROM delta_lake.logs
WHERE timestamp >= current_date()
GROUP BY user_id
ORDER BY page_views DESC
LIMIT 10
```

### Rilevamento Anomalie
```sql
SELECT 
  endpoint, source, level,
  count(*) as anomaly_count,
  max(response_time) as max_response_time
FROM delta_lake.logs 
WHERE (level = 'ERROR' OR response_time > 1000)
  AND timestamp >= current_timestamp() - interval 30 minutes
GROUP BY endpoint, source, level
ORDER BY anomaly_count DESC
```

## 🚨 Troubleshooting

### Connectivity Issues
1. **Kafka unreachable**: Ensure Kafka runs on localhost:9092
2. **Spark unavailable**: Ensure Spark is active on localhost:7077
3. **Delta Lake inaccessible**: Verify permissions and configuration

### Debug
- Check server logs: `npm run server`
- Verify SSE: `curl http://localhost:4000/health`
- Test APIs: `curl http://localhost:4000/api/metrics`

## 📈 Metrics and Performance

The platform monitors in real-time:
- **Throughput**: Events processed per second
- **Latency**: Average response time
- **Error Rate**: Error percentage
- **Active Sessions**: Active user sessions
- **Data Processed**: Volume processed

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## 📄 License

This project is licensed under the MIT License. See `LICENSE` for details.

## 🆘 Support

For support and questions:
- Open a GitHub issue
- Contact the development team
- Check the pipeline documentation in `scripts/`
