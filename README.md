Certo, ecco il README completo con le aggiunte richieste, in modo compatto e integrato con la struttura esistente.

# Real-Time Data Pipeline Platform

An interactive platform to ingest, process, and analyze real-time web logs using Kafka, Spark Structured Streaming, and Delta Lake. The repository includes scripts to run the end-to-end pipeline, optional ML-based anomaly detection, and a simple server for UI streaming.

## Key Features

### Real-Time Updates

  - **Live Log Stream**: Real-time view of logs generated by the pipeline
  - **Dynamic Metrics**: Automatic refresh of metrics based on real data
  - **Interactive Charts**: Charts update on new incoming data
  - **SSE Connection**: Server-Sent Events for instant updates

### Real Pipeline Data

  - **Kafka Integration**: Direct connection to Kafka topics
  - **Spark SQL Queries**: Execute queries against Delta Lake data
  - **Delta Lake Analytics**: Query Delta tables directly
  - **No Mock Data**: All data comes from the real pipeline

### Interactive Interface

  - **Connection Status**: Visual indicators of pipeline health
  - **Query Builder**: UI to run custom Spark SQL queries
  - **Advanced Filters**: Filter logs by level, source, endpoint
  - **Anomaly Detection**: Automatic anomaly detection on data

## Architecture

```
        Web Logs (Sources)
        |
        v
        Kafka (Broker)
        |
        v
        Spark Streaming (Processing)
        |
        v
        Delta Lake (Storage)
        |
        v
        API Server (Express)
        |
        v
        Web Platform (React App)
```

## Singleton Spark Session

To ensure stable Spark execution, the platform uses a singleton pattern for managing Spark sessions. This avoids creating multiple SparkContext instances within the same JVM, which would otherwise trigger the common Spark error “Only one SparkContext may be running in this JVM (SPARK-2243)”.

The shared session logic is implemented in `spark_session_manager.py`. All components of the pipeline (streaming processor, ML processor, anomaly detector, and analytics jobs) import the `get_spark()` function to obtain the same SparkSession instance.

This ensures efficient resource use, stable execution, and consistent Delta Lake + Kafka configuration across jobs.

Spark configuration options such as `SPARK_UI_ENABLED`, `SPARK_UI_PORT`, `SPARK_PORT_MAX_RETRIES`, and `SPARK_LOCAL_IP` can be set via environment variables for flexible runtime behavior.

Example usage:

```bash
from spark_session_manager import get_spark, stop_spark

# Get or create the shared SparkSession
spark = get_spark("LogStreamApp")

# Use Spark normally
df = spark.read.format("delta").load("/tmp/delta-lake/logs")
df.show()

# Stop the session explicitly when done
stop_spark()
```

This design guarantees that all Spark-based components in the platform run on a single, consistent session, making the pipeline more robust and easier to manage.

## Technologies

### Frontend

  - **React 18** with TypeScript
  - **Vite** for build and development
  - **shadcn/ui** for UI components
  - **Tailwind CSS** for styling
  - **Recharts** for interactive charts
  - **React Query** for state management

### Backend

  - **Node.js** with Express
  - **KafkaJS** for Kafka connectivity
  - **Server-Sent Events** for real-time streaming
  - **REST API** for queries and metrics

### Data Pipeline

  - **Apache Kafka** for message streaming
  - **Apache Spark** for processing
  - **Delta Lake** for storage
  - **Spark SQL** for querying

## Installation & Setup

### Prerequisites

  - Node.js 18+ and npm
  - Docker & Docker Compose (to start Kafka and related services)

### 1\. Clone the repository

```bash
git clone <YOUR_REPO_URL>
cd data-pipeline-platform
```

### 2\. Install dependencies

```bash
npm install
```

### 3\. Configure environment variables

```bash
cp .env.example .env
```

Edit the `.env` file with your configuration:

```env
# API Configuration
VITE_API_URL=http://localhost:4000

# Kafka Configuration
KAFKA_BROKERS=localhost:9092
KAFKA_TOPIC=web-logs
KAFKA_GROUP_ID=ui-bridge-group

# Spark Configuration
SPARK_MASTER=spark://localhost:7077
DELTA_LAKE_PATH=/tmp/delta-lake

# Server Configuration
PORT=4000
```

### 4\. Start the backend server (optional)

```bash
npm run server
```

### 5\. Start the frontend app (optional)

```bash
npm run dev
```

-----

## Pipeline Configuration and Usage

### 1\. Docker Setup

The project uses `docker-compose.yml` to orchestrate **Zookeeper**, **Kafka**, and **Kafka UI**. The `setup_environment.sh` and `start_pipeline.sh` scripts handle this automatically, so you don't need to run Docker commands manually.

  * **Zookeeper**: Runs on port `2181` to manage Kafka brokers.
  * **Kafka**: The message broker running on ports `9092` and `29092`.
  * **Kafka UI**: A web interface for monitoring Kafka, accessible at `http://localhost:8080`.

### 2\. `setup_environment.sh`

This script automates the initial setup. Run it once to:

  * Create necessary data and checkpoint directories.
  * Install Python dependencies from `requirements.txt`.
  * Download and configure Apache Spark.
  * Start the Docker services for Kafka and Zookeeper.
  * Automatically create the `web-logs` topic in Kafka.

Run the script from the terminal:

```bash
bash setup_environment.sh
```

### 3\. `start_pipeline.sh`

This is the main script to start the entire pipeline. It:

  * Checks for Docker and Docker Compose availability.
  * Starts the Kafka infrastructure services.
  * Sets up the Python environment and creates required directories.
  * Launches the main pipeline process in the background.
  * Provides access information for monitoring.
  * Includes a cleanup function to safely stop all processes with `Ctrl+C`.

Run the script from the terminal:

```bash
bash start_pipeline.sh
```

### A. Quick Start (recommended)

From the `scripts` directory:

```bash
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
./setup_environment.sh
python3 pipeline_orchestrator.py --action start
```

This will:

  - Start Kafka services with Docker Compose
  - Create Delta/Checkpoint directories
  - Train an anomaly detection model if not present
  - Start the streaming processor, ML streaming processor, and anomaly detector
  - Start the test log generator

To stop:

```bash
python3 pipeline_orchestrator.py --action stop
```

### B. Run components individually

In `scripts/`:

```bash
# 1) Generate logs
python3 enhanced_log_generator.py --rate 10

# 2) Start rule-based streaming
python3 streaming_processor.py --mode stream --output-path /tmp/delta-lake/logs --checkpoint-path /tmp/checkpoints/logs

# 3) Start ML streaming (writes predictions separately)
python3 ml_streaming_processor.py --mode stream --output-path /tmp/delta-lake/logs --ml-output-path /tmp/delta-lake/ml-predictions --checkpoint-path /tmp/checkpoints/logs

# 4) Start anomaly detection
python3 anomaly_detector.py --mode detect --input-path /tmp/delta-lake/logs --output-path /tmp/delta-lake/anomalies --checkpoint-path /tmp/checkpoints/anomalies
```

### C. Analytics and maintenance

```bash
# Batch analytics on rule-based logs
python3 streaming_processor.py --mode analytics --output-path /tmp/delta-lake/logs

# ML analytics on predictions
python3 ml_streaming_processor.py --mode analytics --output-path /tmp/delta-lake/ml-predictions

# Analyze historical anomalies
python3 anomaly_detector.py --mode analyze --output-path /tmp/delta-lake/anomalies

# Optional: delta optimization
python3 streaming_processor.py --mode optimize --output-path /tmp/delta-lake/logs
```

### D. Spark configuration and ports

The scripts use a shared Spark session per process to avoid multiple SparkContext errors and disable the Spark UI by default. You can override with:

```bash
SPARK_UI_ENABLED=true SPARK_UI_PORT=4040 python3 ml_streaming_processor.py --mode stream
```

You can also increase port retries via `SPARK_PORT_MAX_RETRIES=64` and set `SPARK_LOCAL_IP`.

## Main Dashboard

### 1\. Main Dashboard

  - **Metrics Grid**: Real-time metrics (events/sec, error rate, response time, etc.)
  - **Analytics Charts**: Interactive charts for traffic and performance
  - **Live Log Stream**: Real-time log stream from the pipeline

### 2\. Query Interface

  - **Spark SQL**: Run custom queries on Delta Lake data
  - **Sample Queries**: Predefined queries for common analyses
  - **Real-time Results**: Live-updating results

### 3\. Monitoring

  - **Connection Status**: Visual indicators of pipeline status
  - **Anomaly Detection**: Automatic error and anomaly detection
  - **Performance Metrics**: Real-time performance monitoring

## Sample queries

### Error Analysis

```sql
SELECT endpoint, count(*) as error_count,
       avg(response_time) as avg_response_time
FROM delta_lake.logs
WHERE status >= 400 AND timestamp >= current_timestamp() - interval 1 hour
GROUP BY endpoint
ORDER BY error_count DESC
LIMIT 5
```

### User Session Analysis

```sql
SELECT
  user_id,
  count(distinct session_id) as sessions,
  count(*) as page_views,
  sum(response_time) / count(*) as avg_session_time
FROM delta_lake.logs
WHERE timestamp >= current_date()
GROUP BY user_id
ORDER BY page_views DESC
LIMIT 10
```

### Anomaly Detection

```sql
SELECT
  endpoint, source, level,
  count(*) as anomaly_count,
  max(response_time) as max_response_time
FROM delta_lake.logs
WHERE (level = 'ERROR' OR response_time > 1000)
  AND timestamp >= current_timestamp() - interval 30 minutes
GROUP BY endpoint, source, level
ORDER BY anomaly_count DESC
```

## Troubleshooting

### Connectivity Issues

1.  **Kafka unreachable**: Ensure Kafka runs on `localhost:9092`.
2.  **Spark unavailable**: Ensure Spark is active on `localhost:7077`.
3.  **Delta Lake inaccessible**: Verify permissions and configuration.

### Debug

  - Check server logs: `npm run server`
  - Verify SSE: `curl http://localhost:4000/health`
  - Test APIs: `curl http://localhost:4000/api/metrics`

## Metrics and performance

The platform monitors in real-time:

  - **Throughput**: Events processed per second
  - **Latency**: Average response time
  - **Error Rate**: Error percentage
  - **Active Sessions**: Active user sessions
  - **Data Processed**: Volume processed

## Contributing

1.  Fork the repository
2.  Create a feature branch (`git checkout -b feature/AmazingFeature`)
3.  Commit changes (`git commit -m 'Add some AmazingFeature'`)
4.  Push the branch (`git push origin feature/AmazingFeature`)
5.  Open a Pull Request

## License

This project is licensed under the MIT License. See `LICENSE` for details.

## Support

For support and questions:

  - Open a GitHub issue
  - Contact the development team
  - Check the pipeline documentation in `scripts/`